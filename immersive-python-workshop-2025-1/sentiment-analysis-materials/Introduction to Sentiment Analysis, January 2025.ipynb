{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1lU8RZCfNCQO-P_q4rRkJleNkKM94MUQR","timestamp":1733765220164},{"file_id":"1ZtsTpXqaHXBFtQ8s6lq25VAkYEn7Dqyx","timestamp":1723601885153}],"mount_file_id":"1ZtsTpXqaHXBFtQ8s6lq25VAkYEn7Dqyx","authorship_tag":"ABX9TyO6PekrtZhQCQkZE24F6NZb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["First, we'll download and format our data.\n","We are using a modified version of data sourced from this link: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/\n","\n","Download the dataset we'll be using here: https://drive.google.com/file/d/1F-7T3erdExrqd0W-tiZMDT-0GPUdQ8kZ/view?usp=sharing\n","\n","# If you run into a cetificate error with NLTK when running code locally, please try this workaround to download packages locally using the NLTK downloader (run this code in your local terminal, not in Colab):\n"],"metadata":{"id":"6FHO8aboT6Ou"}},{"cell_type":"code","source":["# import nltk\n","# import ssl\n","\n","# try:\n","#     _create_unverified_https_context = ssl._create_unverified_context\n","# except AttributeError:\n","#     pass\n","# else:\n","#     ssl._create_default_https_context = _create_unverified_https_context\n","\n","# nltk.download('vader_lexicon')"],"metadata":{"id":"19klQxJc0I3W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Processing Our Data\n","\n","# First, import Pandas and JSON\n","import pandas as pd\n","import json\n","\n","# Open the file, load it using the json package, then create a dataframe in Pandas using the json_normalize function.\n","file = open('sample_data/Software_array_small.json')\n","data = json.load(file)\n","\n","df = pd.json_normalize(data)\n","print(df)"],"metadata":{"id":"SoA71CO_8I9V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's simplify our data by trimming the components of the dataframe we won't be using.\n","cleaned_df = df[['overall','summary', 'reviewText']]\n","print(cleaned_df)"],"metadata":{"id":"ReAccloEKEAO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Starting Out with Sentiment Analysis\n","\n","# import the Vader package from NLTK\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","\n","# Download the lexicon used by Vader\n","import nltk\n","nltk.download('vader_lexicon')\n","\n","# pandas configuration\n","pd.options.mode.chained_assignment = None\n","\n","# Loading our sentiment analyzer\n","sent_an = SentimentIntensityAnalyzer()\n","\n","# A function that returns our Vader sentiment scores\n","def vader_sentiment(text):\n","    return sent_an.polarity_scores(text)['compound']\n","\n","# Create new column for the Vader compound sentiment score\n","cleaned_df['sentiment score'] = cleaned_df['reviewText'].apply(vader_sentiment)\n","\n","# Categorize our sentiment values as positive, negative,or neutral based on specified score thresholds\n","def categorize_sentiment(sentiment, neg_threshold=-0.05, pos_threshold=0.05):\n","    if sentiment < neg_threshold:\n","        label = 'negative'\n","    elif sentiment > pos_threshold:\n","        label = 'positive'\n","    else:\n","        label = 'neutral'\n","    return label\n","\n","# Create a new column in our dataframe based on our categorized sentiment scores\n","cleaned_df['sentiment classification'] = cleaned_df['sentiment score'].apply(categorize_sentiment)\n","\n","print(cleaned_df)"],"metadata":{"id":"mX4z_dq6Kf1J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let's get the mean of our sentiment scores."],"metadata":{"id":"7vOIvpTYBOMM"}},{"cell_type":"code","source":["sentiment_mean = cleaned_df['sentiment score'].mean()\n","print(sentiment_mean)"],"metadata":{"id":"YatYv1StBS1z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And the counts for how many positive or negative reviews we found."],"metadata":{"id":"kNUFbs7aCLb5"}},{"cell_type":"code","source":["cleaned_df.groupby(['sentiment classification']).size().reset_index(name='count')"],"metadata":{"id":"3_WRfpxtCOrz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that we've performed sentiment analysis on our dataset, let's try a hands-on exercise to practice using Vader on a simple array of sentences. Try completing the code below to perform sentiment analysis on the list of five sentences.\n","\n","# To see example solutions, following this link: https://colab.research.google.com/drive/1C4X8bxkp4OX62Vj1mLC0t9IFZBfnKBnc?usp=sharing"],"metadata":{"id":"HDzeePzuArgt"}},{"cell_type":"code","source":["# importing the necessary packages\n","import nltk\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","nltk.download('vader_lexicon')\n","# Loading our sentiment analyzer\n","sent_an = SentimentIntensityAnalyzer()\n","\n","example_sentences = [\"As I said above, if you're a novice, a relative newcomer or just an experienced web designer who wants a refresher course, this is a good way to do it.\",\n","                     \"Kinda dumb UI choice, initially confusing.\",\n","                     \"If you would like to dive into the publishing aspects and really optimizing your workflow, then this would be a good investment for you.\",\n","                     \"The exposition is clear, but perhaps a bit dry.\",\n","                     \"The software is well known and excellent.\"]\n","\n","# your code here..."],"metadata":{"id":"gfbj93L-A8dd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TextBlob\n","\n","# First, install the package.\n","!pip install textblob"],"metadata":{"id":"KaWjn3GhsA2B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["TextBlob's sentiment property returns a namedtuple of the form Sentiment(polarity, subjectivity). The polarity score is a float within the range [-1.0, 1.0], with -1 being a negative sentiment and 1 being positive. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.\n","\n","First, let's try out the package on a simple sample sentence."],"metadata":{"id":"ilyoCvA02jTa"}},{"cell_type":"code","source":["# importing the package\n","from textblob import TextBlob\n","\n","testimonial1 = TextBlob(\"Textblob is amazingly simple to use. What great fun!\")\n","\n","testimonial2 = TextBlob(\"Textblob is horrendous. What a terrible package!\")\n","\n","print(\"Testimonial 1:\")\n","print(testimonial1.sentiment)\n","print(testimonial1.sentiment.polarity)\n","print(\"\\n\")\n","print(\"Testimonial 2:\")\n","print(testimonial2.sentiment)\n","print(testimonial2.sentiment.polarity)"],"metadata":{"id":"oIMX3tnh2ftM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we can use the sentiment_assessments feature to get additional details about the text after applying sentiment analysis to it.  The sentiment_assessments feature allows us to return data about the sentence  in the form (polarity, subjectivity, assessments), where polarity is a float within the range [-1.0, 1.0]; subjectivity is a float within the range [0.0, 1.0], where 0.0 is very objective and 1.0 is very subjective; and assessments is a list of polarity and subjectivity scores for the assessed tokens."],"metadata":{"id":"FZxRl-f_7QDM"}},{"cell_type":"code","source":["from textblob import TextBlob\n","\n","testimonial = TextBlob(\"Textblob is amazingly simple to use. What great fun!\")\n","\n","print(testimonial.sentiment_assessments.subjectivity)\n","print(testimonial.sentiment_assessments.assessments)"],"metadata":{"id":"ctjDORjF7XgW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, let's try applying TextBlob to the text in our dataframe. This is our first hands-on exercise--try to complete the code below to use Text Blob for sentiment analysis on our reviewText column, like we did with Vader earlier."],"metadata":{"id":"dOi9RgE070ll"}},{"cell_type":"code","source":["# A function that returns our Text Blob sentiment scores\n","def textblob_sentiment(text):\n","    tb_sentiment = TextBlob(text)\n","    return tb_sentiment.sentiment.polarity\n","\n","# Create a new column for the Text Blob compound sentiment score\n","cleaned_df['text blob sentiment score'] = cleaned_df['reviewText'].apply(textblob_sentiment)\n","\n","# Categorize our sentiment values as positive, negative,or neutral based on specified score thresholds\n","def categorize_sentiment(sentiment, neg_threshold=-0.05, pos_threshold=0.05):\n","    if sentiment < neg_threshold:\n","        label = 'negative'\n","    elif sentiment > pos_threshold:\n","        label = 'positive'\n","    else:\n","        label = 'neutral'\n","    return label\n","\n","# Create a new column in our dataframe based on our categorized sentiment scores\n","cleaned_df['text blob sentiment classification'] = cleaned_df['text blob sentiment score'].apply(categorize_sentiment)\n","\n","print(cleaned_df)"],"metadata":{"id":"7HBvfGLw78Fn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Finally, let's see what it looks like to train our own sentiment classifier.\n","# This can be especially useful when working with highly specific data or non-English languages.\n","\n","# imports\n","import nltk\n","from nltk import word_tokenize\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","\n","tb_grouped_df = cleaned_df.groupby('text blob sentiment classification')\n","tb_pos_group = tb_grouped_df.get_group('positive')\n","\n","# First, we provide our training data as a list of sentences and their sentiment classifications:\n","training_data = [(\"It's practical to hope because the hope is for us to survive as a human species.\", \"pos\"),\n","          (\"Italy's soccer players were unlucky to find themselves in a qualifying group with Spain\", \"neg\"),\n","          (\"Instead, root for the Knights whose fans are childlike in their glee.\", \"pos\"),\n","          (\"The overwhelming perception is that the European Union has done very little to alleviate this specifically Italian difficulty.\", \"neg\"),\n","          (\"We are in for a period of painful social conflict, at the end of which perhaps we may remember why it once seemed wise to relegate certain emotions to the stadium.\", \"neg\"),\n","          (\"The heartlessness of that is mind-boggling.\", \"neg\"),\n","          (\"My father was an insatiable learner with intelligence that his baby brother once told me bordered on genius.\", \"pos\"),\n","        ]\n","\n","# Training NLTK's Na√Øve Bayes Classifier with a dictionary created from our list\n","dictionary = set(word.lower() for passage in training_data for word in word_tokenize(passage[0]))\n","\n","t = [({word: (word in word_tokenize(x[0])) for word in dictionary}, x[1]) for x in training_data]\n","\n","classifier = nltk.NaiveBayesClassifier.train(t)\n","\n","# Let's test out our new classifier on a small sample of our positive reviews:\n","random_pos_sample_2 = tb_pos_group.sample(10)\n","sample_texts = random_pos_sample_2['reviewText']\n","\n","# Looping through our list of reviews and outputting the classifier's assessments.\n","for s in sample_texts:\n","    test_data = s\n","    test_data_features = {word.lower(): (word in word_tokenize(test_data.lower())) for word in dictionary}\n","\n","    print(s[:100])\n","    print(\"Classification: \" + classifier.classify(test_data_features))"],"metadata":{"id":"y_DJT9j2QBNk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now, let's visualize our data\n","\n","# Import the matplotlib library\n","import matplotlib.pyplot as plt\n","\n","# Specify the details of our desired plot\n","plt.figure(figsize=(12,12))\n","# creating the figure and single axis\n","fig,ax = plt.subplots()\n","# setting the limit for the y-axis\n","plt.ylim(0, 20)\n","# creating the graph\n","graph_image = cleaned_df['sentiment score'].value_counts().plot(kind='bar', ylabel='frequency')\n","# hiding the x-axis\n","graph_image.axes.get_xaxis().set_visible(False)\n","\n","# save the file\n","from google.colab import files\n","plt.savefig(\"example.png\")\n","files.download(\"example.png\")\n","\n","# displaying the figure\n","plt.show()"],"metadata":{"id":"7snWKey-Nu3E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's try another visualization where we look at a smaller subset of our data\n","# We'll get a random sample of 50 entries in our dataframe and plot those\n","\n","# Taking our sample\n","random_sample = cleaned_df.sample(50)\n","print(random_sample)\n","\n","# Specifying our plot details again\n","plt.figure(figsize=(8,8))\n","fig,ax = plt.subplots()\n","random_sample['sentiment classification'].value_counts().plot(ax = ax, kind = 'bar', ylabel = 'frequency')\n","plt.show()"],"metadata":{"id":"zK4gx3xeP3ts"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For our third hands-on exercise, try to create your own plot using the sentiment scores we generated using Text Blob."],"metadata":{"id":"8DpNoNDBEARY"}},{"cell_type":"code","source":["# Import the matplotlib library\n","import matplotlib.pyplot as plt\n","\n","# your code here..."],"metadata":{"id":"HNNRrtpeDERX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now let's visualize the text itself\n","# We'll stick with a small, random sample of our data to keep things simple\n","\n","# First, we create a new dataframe by grouping our data based on its sentiment classification.\n","# For this example, we'll only look at reviews classified as positive\n","grouped_df = cleaned_df.groupby('sentiment classification')\n","pos_group = grouped_df.get_group('positive')\n","random_pos_sample = pos_group.sample(50)\n","print(random_pos_sample)"],"metadata":{"id":"z9bwrZFBcMSF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now we'll create a wordcloud based on word frequencies in our positive reviews.\n","# First, we get the data ready for visualization...\n","\n","# We remove punctuation from the dataframe using a regular expression\n","random_pos_sample['wcText'] = random_pos_sample['reviewText'].str.replace('[^\\w\\s]','')\n","\n","print(random_pos_sample['wcText'])\n","\n","# Next we remove the stopwords (these can be modified to include non-english languages)\n","nltk.download('stopwords')\n","\n","from nltk.corpus import stopwords\n","stopwords = stopwords.words('english')\n","# You can add custom stopwords, as well:\n","# stopwords.append('example')\n","\n","# Make all of the words in our 'Tokenized Text' column lowercase\n","random_pos_sample['wcText'] = random_pos_sample['wcText'].str.lower()\n","\n","# A lambda function is a small, anonymous function.\n","# A lambda function can take any number of arguments, but can only have one expression.\n","# Here, we're joining the words not in the stopwords list into a single string.\n","# This prepares the text for tokenization.\n","random_pos_sample['wcText'] = random_pos_sample['wcText'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n","\n","\n","# Next, tokenize the text into words using NLTK\n","from nltk import word_tokenize\n","nltk.download('punkt')\n","\n","random_pos_sample['wcText'] = random_pos_sample['wcText'].apply(word_tokenize)\n","\n","\n","# Now we can count the word frequencies in our \"Tokenized Text\" column\n","exploded_text = random_pos_sample['wcText'].explode() # Transform each element of a list-like to a row, replicating index values.\n","print(exploded_text)\n","\n","word_counts = exploded_text.value_counts()\n","print(word_counts)\n","\n","\n","# Bar graph\n","import matplotlib.pyplot as plt\n","\n","word_counts.head(10).plot(x=\"Words\", y=\"Count\", kind=\"bar\", figsize=(10, 9))\n","plt.show()\n","\n","\n","# Word cloud\n","from wordcloud import WordCloud\n","\n","# Convert our exploded_text list to a string\n","wordcloud_string=(\" \").join(exploded_text)\n","\n","# Generate the word cloud\n","wordcloud = WordCloud(width = 1000, height = 500).generate(wordcloud_string)\n","plt.figure(figsize=(15,8))\n","plt.imshow(wordcloud)\n","plt.axis(\"off\")\n","plt.savefig(\"wordcloud\"+\".png\", bbox_inches='tight') # removes extra whitespace around our figure\n","plt.show()\n","plt.close()"],"metadata":{"id":"1tTWSjrbJoTu"},"execution_count":null,"outputs":[]}]}